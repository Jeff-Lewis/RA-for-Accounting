{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDSkjUHYNRxA",
        "outputId": "fda002ca-d4cc-492f-d223-414294b30113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.19.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
            "Requirement already satisfied: trio~=0.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: outcome in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: webdriver-manager in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
            "Requirement already satisfied: requests in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
            "Requirement already satisfied: python-dotenv in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
            "Requirement already satisfied: packaging in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2024.2.2)\n",
            "Requirement already satisfied: tqdm in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: wget\n",
            "zsh:1: command not found: dpkg\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: google-chrome\n"
          ]
        }
      ],
      "source": [
        "# Install Selenium and WebDriver Manager\n",
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "!pip install tqdm\n",
        "\n",
        "# Install Chrome\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip\n",
        "!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Check the versions of Chrome\n",
        "!google-chrome --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.19.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
            "Requirement already satisfied: trio~=0.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: outcome in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: webdriver-manager in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
            "Requirement already satisfied: requests in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
            "Requirement already satisfied: python-dotenv in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
            "Requirement already satisfied: packaging in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2024.2.2)\n",
            "Requirement already satisfied: tqdm in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
            "/opt/homebrew/bin/brew\n",
            "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
            "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
            "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://dl.google.com/chrome/mac/universal/stable/GGRO/googlechr\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[33mWarning:\u001b[0m No checksum defined for cask 'google-chrome', skipping verification.\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling Cask \u001b[32mgoogle-chrome\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPurging files for version 125.0.6422.113 of Cask google-chrome\u001b[0m\n",
            "\u001b[31mError:\u001b[0m It seems there is already an App at '/Applications/Google Chrome.app'.\n",
            "Google Chrome 125.0.6422.78 \n"
          ]
        }
      ],
      "source": [
        "# MacOS Version\n",
        "# Install Selenium and WebDriver Manager\n",
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "!pip install tqdm\n",
        "\n",
        "# Install Homebrew if not installed\n",
        "!which brew || /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
        "\n",
        "# Install Chrome using Homebrew\n",
        "!brew install --cask google-chrome\n",
        "\n",
        "# Check the version of Chrome\n",
        "!\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\" --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kjOPYBAQQ429"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lib for image recognition\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JawMa235Q3v3"
      },
      "outputs": [],
      "source": [
        "# define driver\n",
        "def setup_driver():\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    # options.add_argument('--headless')\n",
        "    # options.add_argument('--disable-gpu')\n",
        "    # options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "    chrome_options.add_argument('--window-size=1920x1080')\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
        "    chrome_options.add_argument(\"accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\")\n",
        "    chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
        "    chrome_options.add_argument(\"accept-encoding=gzip, deflate, br\")\n",
        "    chrome_options.add_argument(\"upgrade-insecure-requests=1\")\n",
        "    chrome_options.add_argument(\"cache-control=no-cache\")\n",
        "    # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "    driver = webdriver.Chrome(options=chrome_options) # if error, use this version\n",
        "    return driver\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_mc4mQ7fREL6"
      },
      "outputs": [],
      "source": [
        "# Function to perform random sleep to mimic human behavior\n",
        "def random_sleep(min_seconds, max_seconds):\n",
        "    sleep(random.uniform(min_seconds, max_seconds))\n",
        "\n",
        "def debug_page(filename_prefix):\n",
        "    if not os.path.exists('debug_dir'):\n",
        "        os.makedirs('debug_dir')\n",
        "    page_source = driver.page_source\n",
        "    with open(f\"debug_dir/{filename_prefix}_page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(page_source)\n",
        "    driver.save_screenshot(f\"debug_dir/{filename_prefix}_screenshot.png\")\n",
        "    print(f\"Debug info saved: {filename_prefix}_page_source.html and {filename_prefix}_screenshot.png\")\n",
        "\n",
        "# Function to handle the Terms of Use page\n",
        "def handle_terms_of_use():\n",
        "    try:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        accept_button = WebDriverWait(driver, 5).until(\n",
        "            EC.element_to_be_clickable((By.ID, 'ctl00_mainContentArea_disclaimerContent_yesButton'))\n",
        "        )\n",
        "        # debug_page(\"terms_of_use\")\n",
        "        accept_button.click()\n",
        "        random_sleep(1, 2)  # Wait a bit to ensure the page loads completely\n",
        "        # print(\"Accepted Terms of Use\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"No Terms of Use page found or unable to click accept button: \")\n",
        "        # debug_page(\"terms_of_use_error\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UWKcKLx_gQf"
      },
      "source": [
        "## Scraping Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cLvuwlygNWUT"
      },
      "outputs": [],
      "source": [
        "def select_date(input_id, date):\n",
        "    month = date.strftime(\"%b\")\n",
        "    year = date.strftime(\"%Y\")\n",
        "    day = date.strftime(\"%d\").lstrip(\"0\")\n",
        "\n",
        "    input_element = driver.find_element(By.ID, input_id)\n",
        "    if input_element:\n",
        "      input_element.click()\n",
        "    # random_sleep(1, 2)\n",
        "    # debug_page(f\"select_date_{input_id}\")\n",
        "\n",
        "    year_selector = Select(driver.find_element(By.CLASS_NAME, 'ui-datepicker-year'))\n",
        "    if year_selector:\n",
        "      year_selector.select_by_visible_text(year)\n",
        "    # debug_page(f\"select_date_year_{input_id}\")\n",
        "    month_selector = Select(driver.find_element(By.CLASS_NAME, 'ui-datepicker-month'))\n",
        "    if month_selector:\n",
        "      month_selector.select_by_visible_text(month)\n",
        "    # debug_page(f\"select_date_month_{input_id}\")\n",
        "    day_element = driver.find_element(By.XPATH, f\"//a[@data-date='{day}']\")\n",
        "    if day_element:\n",
        "      day_element.click()\n",
        "    # debug_page(f\"select_date_day_{input_id}\")\n",
        "\n",
        "# Function to select the necessary checkboxes\n",
        "def select_filings(filings):\n",
        "    # for filing in filings:\n",
        "    #   try:\n",
        "    #       label = driver.find_element(By.XPATH, f'//label[text()=\"{filing}\"]')\n",
        "    #       checkbox = label.find_element(By.XPATH, './preceding-sibling::input[@type=\"checkbox\"]')\n",
        "    #       # print(label.text)\n",
        "    #       checkbox.click()\n",
        "    #       # print(f\"Selected filing: {filing}\")\n",
        "    #   except Exception as e:\n",
        "    #       print(f\"Unable to select filing {filing}: {e}\")\n",
        "    #       debug_page(f\"select_filing_error_{filing}\")\n",
        "    \n",
        "    # for one checkbox only\n",
        "    try:\n",
        "        label = driver.find_element(By.XPATH, f'//label[text()=\"{filings}\"]')\n",
        "        checkbox = label.find_element(By.XPATH, './preceding-sibling::input[@type=\"checkbox\"]')\n",
        "        # print(label.text)\n",
        "        checkbox.click()\n",
        "        # print(f\"Selected filing: {filing}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unable to select filing {filings}: {e}\")\n",
        "        debug_page(f\"select_filing_error_{filings}\")\n",
        "\n",
        "# Function to perform the search\n",
        "def perform_search(start_date, end_date, filings):\n",
        "  print('=' * 50)\n",
        "  print(f\"scraping {filings} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "  try:\n",
        "      driver.get('https://emma.msrb.org/Search/Search.aspx')\n",
        "      handle_terms_of_use()  # Call the function to handle Terms of Use\n",
        "\n",
        "      # Click on the Disclosures tab\n",
        "      try:\n",
        "          disclosures_tab = WebDriverWait(driver, 2).until(\n",
        "              EC.element_to_be_clickable((By.ID, 'disclosuresFilterLi'))\n",
        "          )\n",
        "          disclosures_tab.click()\n",
        "          # print('clicked on disclosures tab')\n",
        "          # random_sleep(2, 4)\n",
        "          # debug_page(\"disclosures_tab\")\n",
        "      except Exception as e:\n",
        "          print(f\"Unable to locate or click the Disclosures tab\")\n",
        "          # debug_page(\"disclosures_tab_error\")\n",
        "          print('try terms_of_use')\n",
        "          if handle_terms_of_use():\n",
        "            df = perform_search(start_date, end_date, filings)\n",
        "            return df\n",
        "          else:\n",
        "            print('=' * 50)\n",
        "            print('error when handling terms of use')\n",
        "            return\n",
        "      # Input the start and end dates\n",
        "\n",
        "      select_date('postingDateFrom', start_date)\n",
        "      select_date('postingDateTo', end_date)\n",
        "\n",
        "      # Select the necessary checkboxes\n",
        "      select_filings(filings)\n",
        "      \n",
        "      # debug_page(\"checkbox\")\n",
        "      # Click the \"Run Search\" button\n",
        "      run_search_button = driver.find_element(By.ID, 'runSearchButton')\n",
        "      run_search_button.click()\n",
        "      random_sleep(2, 4)\n",
        "      # Wait for the counter label to appear\n",
        "      search_results_loaded = WebDriverWait(driver, 30).until(\n",
        "          EC.presence_of_element_located((By.CLASS_NAME, 'searchResultsSecurityView'))\n",
        "      )\n",
        "\n",
        "      print('*** page loaded')\n",
        "      # debug_page(\"run_search\")\n",
        "      length_selector = Select(WebDriverWait(driver, 10).until(\n",
        "      EC.presence_of_element_located((By.NAME, 'lvDocuments_length'))\n",
        "      ))\n",
        "      length_selector.select_by_value('100')\n",
        "      # debug_page(\"length_select\")\n",
        "      counter_label = WebDriverWait(driver, 10).until(\n",
        "          EC.presence_of_element_located((By.ID, 'counterLabel'))\n",
        "      )\n",
        "      num_disclosures = counter_label.text.split()[0]\n",
        "      print(f\"Number of disclosures: {num_disclosures}\")\n",
        "      if num_disclosures == '0':\n",
        "          # print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "          return None\n",
        "      df = scrape_results(num_disclosures)\n",
        "      if not os.path.exists(f'results/{filings}'):\n",
        "        os.makedirs(f'results/{filings}')\n",
        "      filename = f\"results/{filings}/results_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
        "      df.to_csv(filename, index=False)\n",
        "      print('*'*10, filename, 'Saved')\n",
        "      return df\n",
        "  except Exception as e:\n",
        "      print('*'*10, f\"Error during perform_search from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "      debug_page(f\"perform_search_error_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}\")\n",
        "      return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OMaWI4mQDrzj"
      },
      "outputs": [],
      "source": [
        "# Function to scrape the search results\n",
        "def scrape_results(num_disclosures):\n",
        "    results = []\n",
        "    page = 1\n",
        "    try:\n",
        "        pbar = tqdm(total=int(num_disclosures), desc='Scraping results', unit='rows')\n",
        "        while True:\n",
        "            print('scraping page: ', page)\n",
        "            WebDriverWait(driver, 20).until(\n",
        "                EC.visibility_of_element_located((By.ID, 'lvDocuments_wrapper'))\n",
        "            )\n",
        "            # debug_page(\"scrape_results\")\n",
        "\n",
        "            # Scrape the search results from the current page\n",
        "            rows = driver.find_elements(By.CSS_SELECTOR, '#lvDocuments tbody tr')\n",
        "            # print('Number of rows: ', len(rows))\n",
        "            for row in rows:\n",
        "                issuer_name = row.find_element(By.CSS_SELECTOR, 'td:nth-child(1)').text\n",
        "                disclosure_desc = row.find_element(By.CSS_SELECTOR, 'td:nth-child(2) a').text\n",
        "                link = row.find_element(By.CSS_SELECTOR, 'td:nth-child(2) a').get_attribute('href')\n",
        "                date_posted = row.find_element(By.CSS_SELECTOR, 'td:nth-child(3)').text\n",
        "                results.append({\n",
        "                    'issuer_name': issuer_name,\n",
        "                    'disclosure_desc': disclosure_desc,\n",
        "                    'link': link,\n",
        "                    'date_posted': date_posted\n",
        "                })\n",
        "                # print(f\"Issuer: {issuer_name}, Description: {disclosure_desc}, Link: {link}, Date: {date_posted}\")\n",
        "\n",
        "            pbar.update(len(rows))\n",
        "            # print(f\"{(page - 1)* 100 + len(rows)} / {num_disclosures} Saved\")\n",
        "\n",
        "            # Check if there is a next page\n",
        "            next_button = driver.find_element(By.ID, 'lvDocuments_next')\n",
        "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
        "                break  # Exit loop if next button is disabled\n",
        "\n",
        "            next_button.click()  # Go to the next page\n",
        "            page += 1\n",
        "            random_sleep(1, 2)  # Wait for the next page to load\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping results: {e}\")\n",
        "        debug_page(\"scrape_results_error\")\n",
        "    pbar.close()\n",
        "    print(\"\\n\")\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bc7Y5YDApZNK",
        "outputId": "930043f7-5f8b-46f1-a2a7-428b23e17db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "scraping Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party from 2024-04-24 to 2024-04-25\n",
            "*** page loaded\n",
            "Number of disclosures: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping results: 100%|██████████| 2/2 [00:00<00:00, 29.95rows/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scraping page:  1\n",
            "\n",
            "\n",
            "********** results/Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party/results_2024-04-24_2024-04-25.csv Saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "scraping Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party from 2024-04-26 to 2024-04-26\n",
            "*** page loaded\n",
            "Number of disclosures: 0\n",
            "No results found from 2024-04-26 to 2024-04-26\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "filings = [\n",
        "    \"Annual Financial Information and Operating Data\",\n",
        "    \"Audited Financial Statements or ACFR\",\n",
        "    \"Budget\",\n",
        "    \"Change in Accounting Standard\",\n",
        "    \"Change in Fiscal Year / Timing of Annual Disclosure\",\n",
        "    \"Consultant Reports\",\n",
        "    \"Failure to Provide Annual Financial Information as Required\",\n",
        "    \"Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party\",\n",
        "    \"Interim / Additional Financial Information / Operating Data\",\n",
        "    \"Investment / Debt / Financial Policy\",\n",
        "    \"Other Financial / Operating Data\",\n",
        "    \"Quarterly / Monthly Financial Information\"\n",
        "]\n",
        "\n",
        "# input date range and filings\n",
        "start_date = datetime(2024, 4, 24)\n",
        "end_date = datetime(2024, 4, 26)\n",
        "range = 1 # zero based, 1 means 2 days\n",
        "filing = filings[7] # select one filing\n",
        "\n",
        "\n",
        "# run the scraper\n",
        "df_list = []\n",
        "while start_date <= end_date:\n",
        "  driver = setup_driver()\n",
        "  mid_date = start_date + timedelta(days=range) if start_date + timedelta(days=range) < end_date else end_date\n",
        "  df = perform_search(start_date, mid_date, filing)\n",
        "  if df is not None:\n",
        "    df_list.append(df)\n",
        "  else:\n",
        "    print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {mid_date.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "  start_date = mid_date + timedelta(days=1)\n",
        "  driver.quit()\n",
        "if df_list and all(df is not None for df in df_list):\n",
        "  df = pd.concat(df_list)\n",
        "  # df.to_csv(f\"results/{filing}_combined.csv\", index=False)\n",
        "  print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8AKqqnF_RWG"
      },
      "source": [
        "## Scrape detailed page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "n3QHtKXB_Sq8"
      },
      "outputs": [],
      "source": [
        "def read_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def download_pdf(url, file_name):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'application/pdf',\n",
        "        'Referer': 'https://emma.msrb.org/' \n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, stream=True)\n",
        "        response.raise_for_status() \n",
        "        if not os.path.exists('downloaded_pdfs'):\n",
        "            os.makedirs('downloaded_pdfs')\n",
        "        download_dir = os.path.join('downloaded_pdfs', file_name)\n",
        "        with open(download_dir, 'wb') as file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                file.write(chunk)\n",
        "        print(f\"Downloaded {file_name} successfully.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {file_name}. Error: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Failed to save {file_name}. Error: {e}\")\n",
        "        \n",
        "# get cusip text\n",
        "def get_cusip_text(img_url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'application/pdf',\n",
        "        'Referer': 'https://emma.msrb.org/' \n",
        "    }\n",
        "    img_response = requests.get(img_url, headers=headers, stream=True)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "    cusip_text = pytesseract.image_to_string(img, config='--psm 7').strip()\n",
        "    return cusip_text\n",
        "\n",
        "# Scrape detail page\n",
        "def scrape_detail_page(link):\n",
        "    try:\n",
        "        driver.get(link)\n",
        "        \n",
        "        # Handle the terms of use if it appears\n",
        "        try:\n",
        "            terms_of_use_element = WebDriverWait(driver, 20).until(\n",
        "                EC.presence_of_element_located((By.XPATH, \"//span[contains(text(), 'Municipal Securities Rulemaking Board's Website Terms of Use')]\"))\n",
        "            )\n",
        "            if terms_of_use_element: \n",
        "                handle_terms_of_use()\n",
        "        except Exception as e:\n",
        "            print(f\"No terms of use found\")\n",
        "        print('term_of_use handled')\n",
        "        # Wait for the detail page to load\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, 'card-body'))\n",
        "        )\n",
        "        print('find body')\n",
        "        # click on the page to clear instructions\n",
        "        body = driver.find_element(By.TAG_NAME, 'body')\n",
        "        ActionChains(driver).move_to_element(body).click().perform()\n",
        "        print('clicked')\n",
        "        random_sleep(2, 4)\n",
        "        # Scrape the required information\n",
        "        \n",
        "        '''\n",
        "        Part one\n",
        "        '''\n",
        "        # Extract Filing Type\n",
        "    \n",
        "        filing_type = WebDriverWait(driver, 5).until(\n",
        "            EC.visibility_of_element_located((By.ID, 'discType'))\n",
        "        ).text.strip()\n",
        "        \n",
        "        # # Extract Disclosure Details\n",
        "        # rule_mandated_div = WebDriverWait(driver, 5).until(\n",
        "        #     EC.visibility_of_element_located((By.ID, 'ruleMandatedDiv'))\n",
        "        # ).text.strip()\n",
        "        \n",
        "        # # Extract Posted Date and Associated CUSIPs\n",
        "        # left_blue_box = WebDriverWait(driver, 5).until(\n",
        "        #     EC.visibility_of_element_located((By.ID, 'leftBlueBoxDiv'))\n",
        "        # ).text\n",
        "        # posted_date = left_blue_box.split(\"Posted Date:\")[1].split(\"Associated CUSIPs:\")[0].strip()\n",
        "        # associated_cusips = left_blue_box.split(\"Associated CUSIPs:\")[1].strip()\n",
        "\n",
        "        # disclosure_details = f\"{rule_mandated_div}\\nPosted Date: {posted_date}\\nAssociated CUSIPs: {associated_cusips}\"\n",
        "        disclosure_details_div = WebDriverWait(driver, 5).until(\n",
        "            EC.visibility_of_element_located((By.ID, 'leftBlueBoxDiv'))\n",
        "        )   \n",
        "        disclosure_details = disclosure_details_div.text\n",
        "        \n",
        "        print('disclosure_details:', disclosure_details)\n",
        "        # Extract Contact Information\n",
        "        contact_info_div = WebDriverWait(driver, 5).until(\n",
        "            EC.any_of(\n",
        "                # EC.visibility_of_element_located((By.ID, 'issuerContactDiv')),\n",
        "                # EC.visibility_of_element_located((By.ID, 'submitterContactDiv')),\n",
        "                EC.visibility_of_element_located((By.ID, 'rightBlueBoxDiv'))\n",
        "            )\n",
        "        )   \n",
        "        print(contact_info_div.text)\n",
        "        contact_info = contact_info_div.text\n",
        "        # contact_info = ' '.join([elem.text.strip() for elem in contact_info_div.find_elements(By.TAG_NAME, 'td')])\n",
        "        print('part one done')\n",
        "        '''\n",
        "        Part two\n",
        "        '''\n",
        "        # Extract Document Link\n",
        "        # document_button = driver.find_element(By.XPATH, \"//a[@id='viewDoc']\")\n",
        "        # document_link = document_button.get_attribute('href')\n",
        "        # print(document_link)\n",
        "        # document_name = document_link.split('/')[-1]\n",
        "        \n",
        "        # # download pdf\n",
        "        # download_pdf(document_link, document_name)\n",
        "        \n",
        "        # Extract Document Link\n",
        "        document_links = []\n",
        "        document_names = []\n",
        "        \n",
        "        document_button = driver.find_element(By.XPATH, \"//a[@id='viewDoc']\")\n",
        "\n",
        "        # Check if the document button has a 'href' attribute\n",
        "        if document_button.get_attribute('href'):\n",
        "            document_link = document_button.get_attribute('href')\n",
        "            document_name = document_link.split('/')[-1]\n",
        "            document_links.append(document_link)\n",
        "            document_names.append(document_name)\n",
        "            # download pdf\n",
        "            download_pdf(document_link, document_name)\n",
        "        else:\n",
        "            # Get the help attribute which contains the links\n",
        "            help_attribute = document_button.get_attribute('help')\n",
        "            # Parse the help attribute as HTML\n",
        "            soup = BeautifulSoup(help_attribute, 'html.parser')\n",
        "            # Find all the links in the help attribute\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            # Download each link\n",
        "            for link in links:\n",
        "                document_name = link.split('/')[-1]\n",
        "                document_link = f\"https://emma.msrb.org{link}\"\n",
        "                document_links.append(document_link)\n",
        "                document_names.append(document_name)\n",
        "                # download pdf\n",
        "                download_pdf(document_link, document_name)\n",
        "        print('download_pdf done')\n",
        "        # Click the plus button to reveal the CUSIP image\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\"))\n",
        "        )\n",
        "        plus_button = driver.find_element(By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\")\n",
        "        plus_button.click()\n",
        "        \n",
        "        # Wait for the CUSIP image to be visible\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.visibility_of_element_located((By.XPATH, \"//td/a/img[@data-cusip]\"))\n",
        "        )\n",
        "        \n",
        "        print('cusip image found')\n",
        "        # Extract and recognize text from CUSIP image\n",
        "        cusip_img = driver.find_element(By.XPATH, \"//td/a/img[@data-cusip]\")\n",
        "        img_url = cusip_img.get_attribute('src')\n",
        "        cusip_text = get_cusip_text(img_url)\n",
        "\n",
        "        print(cusip_text)\n",
        "        return {\n",
        "            'filing_type': filing_type,\n",
        "            'disclosure_details': disclosure_details,\n",
        "            'contact_info': contact_info,\n",
        "            'document_links': document_links,\n",
        "            'document_names': document_names,\n",
        "            'cusip_text': cusip_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping detail page {link}: {e}\")\n",
        "        debug_page(f\"{link.split('/')[-1]}_error\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_detail_pages_from_csv(file_path):\n",
        "    df = read_csv(file_path)\n",
        "    detail_data = []\n",
        "    # set up tqdm progress bar\n",
        "    pbar = tqdm(total=len(df), desc='Scraping detail pages', unit='rows')\n",
        "    for index, row in df.iterrows():\n",
        "        link = row['link']\n",
        "        print('='*50)\n",
        "        print('Sraping: ',link)\n",
        "        driver = setup_driver()\n",
        "        detail_info = scrape_detail_page(link)\n",
        "        if detail_info:\n",
        "            detail_data.append(detail_info)\n",
        "        # save temp data\n",
        "        detail_df = pd.DataFrame(detail_data)\n",
        "        detail_df.to_csv(f'temp_detail_data_{file_path}.csv', index=False)\n",
        "        driver.quit()\n",
        "        # Optional: Add a delay to mimic human behavior and avoid detection\n",
        "        random_sleep(2, 4)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    detail_df = pd.DataFrame(detail_data)\n",
        "    return detail_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "LAB6o2MVA53N",
        "outputId": "c6805f09-9f98-4354-d854-aad97d2ce75e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   0%|          | 0/513 [00:00<?, ?rows/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364921\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802482-P21383265-P21823263.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "411890CL8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   0%|          | 1/513 [00:11<1:42:00, 11.95s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364924\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802487-P21383268-P21823267.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "411890CL8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   0%|          | 2/513 [00:25<1:48:15, 12.71s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21338686\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802125-P21357082-P21793917.pdf successfully.\n",
            "Downloaded P21802125-P21357082-P21822933.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "084167G6D1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   1%|          | 3/513 [00:35<1:37:27, 11.46s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364850\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802386-P21383190-P21823184.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "19663CAK7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   1%|          | 4/513 [00:48<1:44:05, 12.27s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365072\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802675-P21383425-P21823432.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "011113445.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   1%|          | 5/513 [00:59<1:40:32, 11.88s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365071\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802674-P21383424-P21823431.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "01728LGK2.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   1%|          | 6/513 [01:12<1:42:42, 12.15s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364913\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802471-P21383256-P21823252.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "031231KY¥4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   1%|▏         | 7/513 [01:26<1:47:50, 12.79s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364975\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802548-P21383320-P21823323.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "031231KY¥4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   2%|▏         | 8/513 [01:37<1:42:48, 12.21s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364913\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802471-P21383256-P21823252.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "031231KY¥4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   2%|▏         | 9/513 [01:52<1:48:46, 12.95s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364975\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802548-P21383320-P21823323.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "031231KY¥4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   2%|▏         | 10/513 [02:03<1:43:46, 12.38s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364867\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364867: Message: \n",
            "Stacktrace:\n",
            "0   chromedriver                        0x0000000102d1a510 chromedriver + 4302096\n",
            "1   chromedriver                        0x0000000102d12e58 chromedriver + 4271704\n",
            "2   chromedriver                        0x000000010294419c chromedriver + 278940\n",
            "3   chromedriver                        0x00000001029862c4 chromedriver + 549572\n",
            "4   chromedriver                        0x00000001029bec5c chromedriver + 781404\n",
            "5   chromedriver                        0x000000010297b004 chromedriver + 503812\n",
            "6   chromedriver                        0x000000010297b9ec chromedriver + 506348\n",
            "7   chromedriver                        0x0000000102ce2558 chromedriver + 4072792\n",
            "8   chromedriver                        0x0000000102ce7004 chromedriver + 4091908\n",
            "9   chromedriver                        0x0000000102cc979c chromedriver + 3970972\n",
            "10  chromedriver                        0x0000000102ce78ec chromedriver + 4094188\n",
            "11  chromedriver                        0x0000000102cbc71c chromedriver + 3917596\n",
            "12  chromedriver                        0x0000000102d04b50 chromedriver + 4213584\n",
            "13  chromedriver                        0x0000000102d04ccc chromedriver + 4213964\n",
            "14  chromedriver                        0x0000000102d12a50 chromedriver + 4270672\n",
            "15  libsystem_pthread.dylib             0x000000019a242f94 _pthread_start + 136\n",
            "16  libsystem_pthread.dylib             0x000000019a23dd34 thread_start + 8\n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   2%|▏         | 11/513 [02:32<2:27:20, 17.61s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365014\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802663-P21383365-P21823371.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "O4052AAW6.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   2%|▏         | 12/513 [02:45<2:14:29, 16.11s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365156\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365156: Message: \n",
            "Stacktrace:\n",
            "0   chromedriver                        0x0000000102d1a510 chromedriver + 4302096\n",
            "1   chromedriver                        0x0000000102d12e58 chromedriver + 4271704\n",
            "2   chromedriver                        0x000000010294419c chromedriver + 278940\n",
            "3   chromedriver                        0x00000001029862c4 chromedriver + 549572\n",
            "4   chromedriver                        0x00000001029bec5c chromedriver + 781404\n",
            "5   chromedriver                        0x000000010297b004 chromedriver + 503812\n",
            "6   chromedriver                        0x000000010297b9ec chromedriver + 506348\n",
            "7   chromedriver                        0x0000000102ce2558 chromedriver + 4072792\n",
            "8   chromedriver                        0x0000000102ce7004 chromedriver + 4091908\n",
            "9   chromedriver                        0x0000000102cc979c chromedriver + 3970972\n",
            "10  chromedriver                        0x0000000102ce78ec chromedriver + 4094188\n",
            "11  chromedriver                        0x0000000102cbc71c chromedriver + 3917596\n",
            "12  chromedriver                        0x0000000102d04b50 chromedriver + 4213584\n",
            "13  chromedriver                        0x0000000102d04ccc chromedriver + 4213964\n",
            "14  chromedriver                        0x0000000102d12a50 chromedriver + 4270672\n",
            "15  libsystem_pthread.dylib             0x000000019a242f94 _pthread_start + 136\n",
            "16  libsystem_pthread.dylib             0x000000019a23dd34 thread_start + 8\n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   3%|▎         | 13/513 [03:14<2:46:59, 20.04s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365156\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365156: Message: \n",
            "Stacktrace:\n",
            "0   chromedriver                        0x0000000102d1a510 chromedriver + 4302096\n",
            "1   chromedriver                        0x0000000102d12e58 chromedriver + 4271704\n",
            "2   chromedriver                        0x000000010294419c chromedriver + 278940\n",
            "3   chromedriver                        0x00000001029862c4 chromedriver + 549572\n",
            "4   chromedriver                        0x00000001029bec5c chromedriver + 781404\n",
            "5   chromedriver                        0x000000010297b004 chromedriver + 503812\n",
            "6   chromedriver                        0x000000010297b9ec chromedriver + 506348\n",
            "7   chromedriver                        0x0000000102ce2558 chromedriver + 4072792\n",
            "8   chromedriver                        0x0000000102ce7004 chromedriver + 4091908\n",
            "9   chromedriver                        0x0000000102cc979c chromedriver + 3970972\n",
            "10  chromedriver                        0x0000000102ce78ec chromedriver + 4094188\n",
            "11  chromedriver                        0x0000000102cbc71c chromedriver + 3917596\n",
            "12  chromedriver                        0x0000000102d04b50 chromedriver + 4213584\n",
            "13  chromedriver                        0x0000000102d04ccc chromedriver + 4213964\n",
            "14  chromedriver                        0x0000000102d12a50 chromedriver + 4270672\n",
            "15  libsystem_pthread.dylib             0x000000019a242f94 _pthread_start + 136\n",
            "16  libsystem_pthread.dylib             0x000000019a23dd34 thread_start + 8\n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   3%|▎         | 14/513 [03:43<3:08:01, 22.61s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364841\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802373-P21383180-P21823172.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "068131873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   3%|▎         | 15/513 [04:00<2:53:38, 20.92s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364826\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364826: Message: \n",
            "Stacktrace:\n",
            "0   chromedriver                        0x0000000102d1a510 chromedriver + 4302096\n",
            "1   chromedriver                        0x0000000102d12e58 chromedriver + 4271704\n",
            "2   chromedriver                        0x000000010294419c chromedriver + 278940\n",
            "3   chromedriver                        0x00000001029862c4 chromedriver + 549572\n",
            "4   chromedriver                        0x00000001029bec5c chromedriver + 781404\n",
            "5   chromedriver                        0x000000010297b004 chromedriver + 503812\n",
            "6   chromedriver                        0x000000010297b9ec chromedriver + 506348\n",
            "7   chromedriver                        0x0000000102ce2558 chromedriver + 4072792\n",
            "8   chromedriver                        0x0000000102ce7004 chromedriver + 4091908\n",
            "9   chromedriver                        0x0000000102cc979c chromedriver + 3970972\n",
            "10  chromedriver                        0x0000000102ce78ec chromedriver + 4094188\n",
            "11  chromedriver                        0x0000000102cbc71c chromedriver + 3917596\n",
            "12  chromedriver                        0x0000000102d04b50 chromedriver + 4213584\n",
            "13  chromedriver                        0x0000000102d04ccc chromedriver + 4213964\n",
            "14  chromedriver                        0x0000000102d12a50 chromedriver + 4270672\n",
            "15  libsystem_pthread.dylib             0x000000019a242f94 _pthread_start + 136\n",
            "16  libsystem_pthread.dylib             0x000000019a23dd34 thread_start + 8\n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   3%|▎         | 16/513 [04:27<3:10:40, 23.02s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364960\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "part one done\n",
            "Downloaded P21802531-P21383304-P21823307.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "O77131KTE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   3%|▎         | 17/513 [04:42<2:49:52, 20.55s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365019\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365019: Message: \n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   4%|▎         | 18/513 [05:10<3:07:28, 22.72s/rows]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365065\n",
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "Error scraping detail page https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21365065: Message: \n",
            "Stacktrace:\n",
            "0   chromedriver                        0x0000000102d1a510 chromedriver + 4302096\n",
            "1   chromedriver                        0x0000000102d12e58 chromedriver + 4271704\n",
            "2   chromedriver                        0x000000010294419c chromedriver + 278940\n",
            "3   chromedriver                        0x00000001029862c4 chromedriver + 549572\n",
            "4   chromedriver                        0x00000001029bec5c chromedriver + 781404\n",
            "5   chromedriver                        0x000000010297b004 chromedriver + 503812\n",
            "6   chromedriver                        0x000000010297b9ec chromedriver + 506348\n",
            "7   chromedriver                        0x0000000102ce2558 chromedriver + 4072792\n",
            "8   chromedriver                        0x0000000102ce7004 chromedriver + 4091908\n",
            "9   chromedriver                        0x0000000102cc979c chromedriver + 3970972\n",
            "10  chromedriver                        0x0000000102ce78ec chromedriver + 4094188\n",
            "11  chromedriver                        0x0000000102cbc71c chromedriver + 3917596\n",
            "12  chromedriver                        0x0000000102d04b50 chromedriver + 4213584\n",
            "13  chromedriver                        0x0000000102d04ccc chromedriver + 4213964\n",
            "14  chromedriver                        0x0000000102d12a50 chromedriver + 4270672\n",
            "15  libsystem_pthread.dylib             0x000000019a242f94 _pthread_start + 136\n",
            "16  libsystem_pthread.dylib             0x000000019a23dd34 thread_start + 8\n",
            "\n",
            "Debug info saved: scrape_detail_page_error_page_source.html and scrape_detail_page_error_screenshot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping detail pages:   4%|▎         | 19/513 [05:40<3:24:04, 24.79s/rows]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the WebDriver\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# driver = setup_driver()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use the function to scrape details from the CSV\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m detail_df \u001b[38;5;241m=\u001b[39m scrape_detail_pages_from_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_2024-04-24_2024-04-24.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(detail_df)\n",
            "Cell \u001b[0;32mIn[56], line 178\u001b[0m, in \u001b[0;36mscrape_detail_pages_from_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# set up tqdm progress bar\u001b[39;00m\n\u001b[1;32m    177\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping detail pages\u001b[39m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    179\u001b[0m     link \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:1411\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1411\u001b[0m     s \u001b[38;5;241m=\u001b[39m klass(v, index\u001b[38;5;241m=\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mk)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m k, s\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:474\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    472\u001b[0m manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 474\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleBlockManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    476\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1934\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(refs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m], axes[\u001b[38;5;241m0\u001b[39m], refs, parent, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1934\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_array\u001b[39m(\u001b[38;5;28mcls\u001b[39m, array: ArrayLike, index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;124;03m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m     block \u001b[38;5;241m=\u001b[39m new_block(array, placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(index)), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize the WebDriver\n",
        "# driver = setup_driver()\n",
        "\n",
        "# Use the function to scrape details from the CSV\n",
        "detail_df = scrape_detail_pages_from_csv('results_2024-04-24_2024-04-24.csv')\n",
        "print(detail_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "detail_df.to_csv('test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No terms of use found\n",
            "term_of_use handled\n",
            "find body\n",
            "clicked\n",
            "disclosure_details: Rule 15c2-12 Disclosure\n",
            "Audited Financial Statements or ACFR: Audited Financial Statement as of 12/31/2023-Hardin Co WD No 2, for the year ended 12/31/2023\n",
            "Posted Date: 04/24/2024\n",
            "Associated CUSIPs: 39\n",
            "Issuer's Contact Information\n",
            "Company: Hardin County Water District No. 2\n",
            "Name: Mandy Isham\n",
            "Address:\n",
            "City, State, Zip:\n",
            "Phone Number: 270-737-1056 ext. 259\n",
            "Email: misham@hcwd2.org\n",
            "Submitter's Contact Information\n",
            "Company: Robert W Baird & Co., Inc.\n",
            "Name: TAMMEY BIBB\n",
            "Address: 500 W JEFFERSON ST\n",
            "City, State, Zip: LOUISVILLE, KY 40202\n",
            "Phone Number: 5025881124\n",
            "Email: tbibb@rwbaird.com\n",
            "part one done\n",
            "Downloaded P21802487-P21383268-P21823267.pdf successfully.\n",
            "download_pdf done\n",
            "cusip image found\n",
            "411890CL8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'filing_type': 'Filing Type: FINANCIAL/OPERATING FILING (CUSIP-9 Based)',\n",
              " 'disclosure_details': 'Rule 15c2-12 Disclosure\\nAudited Financial Statements or ACFR: Audited Financial Statement as of 12/31/2023-Hardin Co WD No 2, for the year ended 12/31/2023\\nPosted Date: 04/24/2024\\nAssociated CUSIPs: 39',\n",
              " 'contact_info': \"Issuer's Contact Information\\nCompany: Hardin County Water District No. 2\\nName: Mandy Isham\\nAddress:\\nCity, State, Zip:\\nPhone Number: 270-737-1056 ext. 259\\nEmail: misham@hcwd2.org\\nSubmitter's Contact Information\\nCompany: Robert W Baird & Co., Inc.\\nName: TAMMEY BIBB\\nAddress: 500 W JEFFERSON ST\\nCity, State, Zip: LOUISVILLE, KY 40202\\nPhone Number: 5025881124\\nEmail: tbibb@rwbaird.com\",\n",
              " 'document_links': ['https://emma.msrb.org/P21802487-P21383268-P21823267.pdf'],\n",
              " 'document_names': ['P21802487-P21383268-P21823267.pdf'],\n",
              " 'cusip_text': '411890CL8'}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scrape_detail_page('https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21364924')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "L1yneIrjJWuC",
        "outputId": "6b253431-46fd-4ebe-a367-8da33ce6b635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "handled\n",
            "cusip image found\n",
            "https://emma.msrb.org/ImageGenerator.ashx?cusip9=A9EB5BB3326932130FBCEFABB8DBDEC64&rowNum=1&isLink=true\n"
          ]
        }
      ],
      "source": [
        "driver = setup_driver()\n",
        "driver.get('https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21338686')\n",
        "\n",
        "# Handle the terms of use if it appears\n",
        "handle_terms_of_use()\n",
        "print('handled')\n",
        "body = driver.find_element(By.TAG_NAME, 'body')\n",
        "ActionChains(driver).move_to_element(body).click().perform()\n",
        "# Wait for the detail page to load\n",
        "WebDriverWait(driver, 10).until(\n",
        "    EC.presence_of_element_located((By.CLASS_NAME, 'card-body'))\n",
        ")\n",
        "WebDriverWait(driver, 10).until(\n",
        "    EC.element_to_be_clickable((By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\"))\n",
        ")\n",
        "plus_button = driver.find_element(By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\")\n",
        "plus_button.click()\n",
        "\n",
        "\n",
        "# Wait for the CUSIP image to be visible\n",
        "WebDriverWait(driver, 10).until(\n",
        "    EC.visibility_of_element_located((By.XPATH, \"//td/a/img[@data-cusip]\"))\n",
        ")\n",
        "\n",
        "print('cusip image found')\n",
        "# Extract and recognize text from CUSIP image\n",
        "cusip_img = driver.find_element(By.XPATH, \"//td/a/img[@data-cusip]\")\n",
        "print(cusip_img.get_attribute('src'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'application/pdf',\n",
        "        'Referer': 'https://emma.msrb.org/' \n",
        "    }\n",
        "\n",
        "img_url = cusip_img.get_attribute('src')\n",
        "img_response = requests.get(img_url, headers=headers, stream=True)\n",
        "img = Image.open(BytesIO(img_response.content))\n",
        "cusip_text = pytesseract.image_to_string(img, config='--psm 7').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "054082AR3,\n"
          ]
        }
      ],
      "source": [
        "print(cusip_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRIM4gK4_W6d"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
