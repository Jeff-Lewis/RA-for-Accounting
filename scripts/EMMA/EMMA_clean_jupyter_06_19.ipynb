{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xDSkjUHYNRxA",
    "outputId": "fda002ca-d4cc-492f-d223-414294b30113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2024.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: pytesseract in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from pytesseract) (9.4.0)\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: dpkg\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: google-chrome\n"
     ]
    }
   ],
   "source": [
    "# Install Selenium and WebDriver Manager\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install tqdm\n",
    "!pip install pytesseract\n",
    "\n",
    "# Install Chrome\n",
    "!apt-get update\n",
    "!apt-get install -y wget unzip\n",
    "!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!dpkg -i google-chrome-stable_current_amd64.deb\n",
    "!apt-get -f install -y\n",
    "\n",
    "# Check the versions of Chrome\n",
    "!google-chrome --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2024.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/mrplugy/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "/opt/homebrew/bin/brew\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://dl.google.com/chrome/mac/universal/stable/GGRO/googlechr\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[33mWarning:\u001b[0m No checksum defined for cask 'google-chrome', skipping verification.\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling Cask \u001b[32mgoogle-chrome\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPurging files for version 125.0.6422.113 of Cask google-chrome\u001b[0m\n",
      "\u001b[31mError:\u001b[0m It seems there is already an App at '/Applications/Google Chrome.app'.\n",
      "Google Chrome 125.0.6422.78 \n"
     ]
    }
   ],
   "source": [
    "# MacOS Version\n",
    "# Install Selenium and WebDriver Manager\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install tqdm\n",
    "\n",
    "# Install Homebrew if not installed\n",
    "!which brew || /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "# Install Chrome using Homebrew\n",
    "!brew install --cask google-chrome\n",
    "\n",
    "# Check the version of Chrome\n",
    "!\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\" --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kjOPYBAQQ429"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib for image recognition\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JawMa235Q3v3"
   },
   "outputs": [],
   "source": [
    "# define driver\n",
    "def setup_driver():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    # options.add_argument('--headless')\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    # options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920x1080')\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "    chrome_options.add_argument(\"accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\")\n",
    "    chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
    "    chrome_options.add_argument(\"accept-encoding=gzip, deflate, br\")\n",
    "    chrome_options.add_argument(\"upgrade-insecure-requests=1\")\n",
    "    chrome_options.add_argument(\"cache-control=no-cache\")\n",
    "    # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver = webdriver.Chrome(options=chrome_options) # if error, use this version\n",
    "    return driver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_mc4mQ7fREL6"
   },
   "outputs": [],
   "source": [
    "# Function to perform random sleep to mimic human behavior\n",
    "def random_sleep(min_seconds, max_seconds):\n",
    "    sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def debug_page(filename_prefix, filings='All'):\n",
    "    directory = f'debug_dir/{filings}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    # page_source = driver.page_source\n",
    "    # with open(f\"{directory}/{filename_prefix}_page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_source)\n",
    "    driver.save_screenshot(f\"{directory}/{filename_prefix}_screenshot.png\")\n",
    "    print(f\"Debug info saved: {filename_prefix}_page_source.html and {filename_prefix}_screenshot.png\")\n",
    "\n",
    "# Function to handle the Terms of Use page\n",
    "def handle_terms_of_use():\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        accept_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'ctl00_mainContentArea_disclaimerContent_yesButton'))\n",
    "        )\n",
    "        # debug_page(\"terms_of_use\")\n",
    "        accept_button.click()\n",
    "        random_sleep(1, 2)  # Wait a bit to ensure the page loads completely\n",
    "        # print(\"Accepted Terms of Use\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"No Terms of Use page found or unable to click accept button: \")\n",
    "        # debug_page(\"terms_of_use_error\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UWKcKLx_gQf"
   },
   "source": [
    "## Scraping Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_search_result_window():\n",
    "    try:\n",
    "        # Wait for the counter label to appear\n",
    "        search_results_loaded = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'searchResultsSecurityView'))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Search results window did not load\")\n",
    "        return False\n",
    "\n",
    "# Detect large search results\n",
    "def is_large_search_result():\n",
    "    try:\n",
    "        large_search_message = driver.find_element(By.ID, 'counterLabel')\n",
    "        return \"Please use filters to narrow your search.\" in large_search_message.text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cLvuwlygNWUT"
   },
   "outputs": [],
   "source": [
    "def select_date(input_id, date):\n",
    "    month = date.strftime(\"%b\")\n",
    "    year = date.strftime(\"%Y\")\n",
    "    day = date.strftime(\"%d\").lstrip(\"0\")\n",
    "\n",
    "    input_element = driver.find_element(By.ID, input_id)\n",
    "    if input_element:\n",
    "        input_element.click()\n",
    "    # random_sleep(1, 2)\n",
    "    # debug_page(f\"select_date_{input_id}\")\n",
    "\n",
    "    year_selector = Select(driver.find_element(By.CLASS_NAME, 'ui-datepicker-year'))\n",
    "    if year_selector:\n",
    "        year_selector.select_by_visible_text(year)\n",
    "    # debug_page(f\"select_date_year_{input_id}\")\n",
    "    month_selector = Select(driver.find_element(By.CLASS_NAME, 'ui-datepicker-month'))\n",
    "    if month_selector:\n",
    "        month_selector.select_by_visible_text(month)\n",
    "    # debug_page(f\"select_date_month_{input_id}\")\n",
    "    day_element = driver.find_element(By.XPATH, f\"//a[@data-date='{day}']\")\n",
    "    if day_element:\n",
    "        day_element.click()\n",
    "    # debug_page(f\"select_date_day_{input_id}\")\n",
    "\n",
    "# Function to select the necessary checkboxes\n",
    "def select_filings(filings):\n",
    "    # for filing in filings:\n",
    "    #   try:\n",
    "    #       label = driver.find_element(By.XPATH, f'//label[text()=\"{filing}\"]')\n",
    "    #       checkbox = label.find_element(By.XPATH, './preceding-sibling::input[@type=\"checkbox\"]')\n",
    "    #       # print(label.text)\n",
    "    #       checkbox.click()\n",
    "    #       # print(f\"Selected filing: {filing}\")\n",
    "    #   except Exception as e:\n",
    "    #       print(f\"Unable to select filing {filing}: {e}\")\n",
    "    #       debug_page(f\"select_filing_error_{filing}\")\n",
    "    \n",
    "    # for one checkbox only\n",
    "    try:\n",
    "        label = driver.find_element(By.XPATH, f'//label[text()=\"{filings}\"]')\n",
    "        checkbox = label.find_element(By.XPATH, './preceding-sibling::input[@type=\"checkbox\"]')\n",
    "        # print(label.text)\n",
    "        checkbox.click()\n",
    "        # print(f\"Selected filing: {filing}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to select filing {filings}: {e}\")\n",
    "        debug_page(f\"select_filing_error_{filings}\")\n",
    "\n",
    "# Function to perform the search\n",
    "def perform_search(start_date, end_date, filings):\n",
    "    \n",
    "    print('=' * 50)\n",
    "    print(f\"scraping {filings} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    def click_disclosures_tab():\n",
    "        try:\n",
    "            disclosures_tab = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.ID, 'disclosuresFilterLi'))\n",
    "            )\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            print('clicked on disclosures tab')\n",
    "            # random_sleep(2, 4)\n",
    "            # debug_page(\"disclosures_tab\"， filings)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to locate or click the Disclosures tab\")\n",
    "            debug_page(\"disclosures_tab_error\", filings)\n",
    "            \n",
    "    # Narrow the search by applying additional filters\n",
    "    def narrow_search_and_scrape():\n",
    "        try:\n",
    "            expand_all_filters = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.ID, 'expandAllFilter'))\n",
    "            )\n",
    "            expand_all_filters.click()\n",
    "            expand_all_filters.click()\n",
    "            expand_all_filters.click()\n",
    "            expand_all_filters.click()\n",
    "            \n",
    "            random_sleep(2, 4)\n",
    "            \n",
    "            filter_dropdown = Select(driver.find_element(By.ID, 'securedByDropdownList'))\n",
    "            options = [option.get_attribute('value') for option in filter_dropdown.options if option.get_attribute('value')]\n",
    "            df_li = []\n",
    "            for option in options:\n",
    "                print(f\"Applying filter: {option}\")\n",
    "                filter_dropdown.select_by_value(option)\n",
    "                random_sleep(2, 4)\n",
    "                \n",
    "                # Click the \"Run Search\" button again\n",
    "                run_search_button = driver.find_element(By.ID, 'runSearchButton')\n",
    "                run_search_button.click()\n",
    "                random_sleep(2, 4)\n",
    "                \n",
    "                # Wait for the search results to load\n",
    "                WebDriverWait(driver, 30).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, 'searchResultsSecurityView'))\n",
    "                )\n",
    "                print(\"Window loaded\")\n",
    "                element = WebDriverWait(driver, 120).until(\n",
    "                    lambda driver: driver.find_element(By.ID, \"tabs\").get_attribute(\"style\") == \"display: block;\"\n",
    "                )\n",
    "                print(f\"Results for filter {option} loaded\")\n",
    "                \n",
    "                # if check_search_result_window():\n",
    "                length_selector = Select(WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.NAME, 'lvDocuments_length'))\n",
    "                ))\n",
    "                length_selector.select_by_value('100')\n",
    "                # debug_page(\"length_select\")\n",
    "                counter_label = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.ID, 'counterLabel'))\n",
    "                )\n",
    "                num_disclosures = counter_label.text.split()[0]\n",
    "                print(f\"Number of disclosures: {num_disclosures}\")\n",
    "                if num_disclosures == '0':\n",
    "                    # print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "                    return None\n",
    "                \n",
    "                df = scrape_results(num_disclosures, filings)\n",
    "                print(f'number of rows: {len(df)}')\n",
    "                df_li.append(df)\n",
    "                expand_all_filters = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.ID, 'expandAllFilter'))\n",
    "                )\n",
    "                \n",
    "                expand_all_filters.click()\n",
    "                expand_all_filters.click()\n",
    "                expand_all_filters.click()\n",
    "                expand_all_filters.click()  \n",
    "            \n",
    "            random_sleep(2, 4)\n",
    "            df = pd.concat(df_li)\n",
    "            if not os.path.exists(f'results/{filings.replace(\"/\", \"_\")}'):\n",
    "                os.makedirs(f'results/{filings.replace(\"/\", \"_\")}')\n",
    "            filename = f\"results/{filings.replace(\"/\", \"_\")}/results_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            print('*'*10, filename, 'Saved')\n",
    "            return df\n",
    "\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during narrow_search_and_scrape: {e}\")\n",
    "            debug_page(\"narrow_search_error_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')\", filings)\n",
    "    try:\n",
    "        driver.get('https://emma.msrb.org/Search/Search.aspx')\n",
    "        # Handle the terms of use if it appears\n",
    "        if len(driver.find_elements(By.CLASS_NAME, 'contentAreaDisclaimer')) > 0:\n",
    "            # print('terms_of_use found')\n",
    "            handle_terms_of_use()\n",
    "\n",
    "        # Click on the Disclosures tab\n",
    "        try:\n",
    "            disclosures_tab = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.ID, 'disclosuresFilterLi'))\n",
    "            )\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            disclosures_tab.click()\n",
    "            print('clicked on disclosures tab')\n",
    "            # random_sleep(2, 4)\n",
    "            # debug_page(\"disclosures_tab\"， filings)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to locate or click the Disclosures tab\")\n",
    "            # debug_page(\"disclosures_tab_error\")\n",
    "            print('try terms_of_use')\n",
    "            if handle_terms_of_use():\n",
    "                df = perform_search(start_date, end_date, filings)\n",
    "                return df\n",
    "            else:\n",
    "                print('=' * 50)\n",
    "                print('error when handling terms of use')\n",
    "                return\n",
    "        try: \n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"//h4[text()='Disclosures']\"))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Disclosures tab not loaded')\n",
    "            click_disclosures_tab()\n",
    "        \n",
    "        # Input the start and end dates\n",
    "\n",
    "        select_date('postingDateFrom', start_date)\n",
    "        select_date('postingDateTo', end_date)\n",
    "\n",
    "        # Select the necessary checkboxes\n",
    "        select_filings(filings)\n",
    "\n",
    "        # debug_page(\"checkbox\")\n",
    "        # Click the \"Run Search\" button\n",
    "        run_search_button = driver.find_element(By.ID, 'runSearchButton')\n",
    "        run_search_button.click()\n",
    "        random_sleep(2, 4)\n",
    "        \n",
    "        print('*** page loaded')\n",
    "        # debug_page(\"run_search\")\n",
    "        if not check_search_result_window():\n",
    "            if is_large_search_result():\n",
    "                print(\"Large search result detected\")\n",
    "                df = narrow_search_and_scrape()\n",
    "                return df\n",
    "        length_selector = Select(WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.NAME, 'lvDocuments_length'))\n",
    "        ))\n",
    "        length_selector.select_by_value('100')\n",
    "        # debug_page(\"length_select\")\n",
    "        counter_label = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.ID, 'counterLabel'))\n",
    "        )\n",
    "        num_disclosures = counter_label.text.split()[0]\n",
    "        print(f\"Number of disclosures: {num_disclosures}\")\n",
    "        if num_disclosures == '0':\n",
    "            # print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "            return None\n",
    "        df = scrape_results(num_disclosures, filings)\n",
    "        if not os.path.exists(f'results/{filings}'):\n",
    "            os.makedirs(f'results/{filings}')\n",
    "        filename = f\"results/{filings}/results_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print('*'*10, filename, 'Saved')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print('*'*10, f\"Error during perform_search from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        debug_page(f\"perform_search_error_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}\", filings)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OMaWI4mQDrzj"
   },
   "outputs": [],
   "source": [
    "# Function to scrape the search results\n",
    "def scrape_results(num_disclosures, filings='All'):\n",
    "    results = []\n",
    "    page = 1\n",
    "    try:\n",
    "        pbar = tqdm(total=int(num_disclosures), desc='Scraping results', unit='rows')\n",
    "        while True:\n",
    "            print('scraping page: ', page)\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.visibility_of_element_located((By.ID, 'lvDocuments_wrapper'))\n",
    "            )\n",
    "            # debug_page(\"scrape_results\")\n",
    "\n",
    "            # Scrape the search results from the current page\n",
    "            rows = driver.find_elements(By.CSS_SELECTOR, '#lvDocuments tbody tr')\n",
    "            # print('Number of rows: ', len(rows))\n",
    "            for row in rows:\n",
    "                issuer_name = row.find_element(By.CSS_SELECTOR, 'td:nth-child(1)').text\n",
    "                disclosure_desc = row.find_element(By.CSS_SELECTOR, 'td:nth-child(2) a').text\n",
    "                link = row.find_element(By.CSS_SELECTOR, 'td:nth-child(2) a').get_attribute('href')\n",
    "                date_posted = row.find_element(By.CSS_SELECTOR, 'td:nth-child(3)').text\n",
    "                results.append({\n",
    "                    'issuer_name': issuer_name,\n",
    "                    'disclosure_desc': disclosure_desc,\n",
    "                    'link': link,\n",
    "                    'date_posted': date_posted\n",
    "                })\n",
    "                # print(f\"Issuer: {issuer_name}, Description: {disclosure_desc}, Link: {link}, Date: {date_posted}\")\n",
    "\n",
    "            pbar.update(len(rows))\n",
    "            # print(f\"{(page - 1)* 100 + len(rows)} / {num_disclosures} Saved\")\n",
    "\n",
    "            # Check if there is a next page\n",
    "            next_button = driver.find_element(By.ID, 'lvDocuments_next')\n",
    "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                break  # Exit loop if next button is disabled\n",
    "\n",
    "            next_button.click()  # Go to the next page\n",
    "            page += 1\n",
    "            random_sleep(1, 2)  # Wait for the next page to load\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping results: {e}\")\n",
    "        debug_page(\"scrape_results_error\", filings)\n",
    "    pbar.close()\n",
    "    print(\"\\n\")\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc7Y5YDApZNK",
    "outputId": "930043f7-5f8b-46f1-a2a7-428b23e17db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "scraping Annual Financial Information and Operating Data from 2021-05-05 to 2021-05-10\n",
      "clicked on disclosures tab\n",
      "No results found from 2021-05-05 to 2021-05-10\n",
      "Error dates found:  1\n",
      "Saved to:  results/Annual Financial Information and Operating Data_error_dates_2021-05-05_2021-05-10.csv\n"
     ]
    }
   ],
   "source": [
    "# old version\n",
    "filings = [\n",
    "    \"Annual Financial Information and Operating Data\",\n",
    "    \"Audited Financial Statements or ACFR\",\n",
    "    \"Budget\",\n",
    "    \"Change in Accounting Standard\",\n",
    "    \"Change in Fiscal Year / Timing of Annual Disclosure\",\n",
    "    \"Consultant Reports\",\n",
    "    \"Failure to Provide Annual Financial Information as Required\",\n",
    "    \"Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party\",\n",
    "    \"Interim / Additional Financial Information / Operating Data\",\n",
    "    \"Investment / Debt / Financial Policy\",\n",
    "    \"Other Financial / Operating Data\",\n",
    "    \"Quarterly / Monthly Financial Information\"\n",
    "]\n",
    "\n",
    "# input date range and filings\n",
    "start_date = datetime(2021, 5, 5)\n",
    "end_date = datetime(2021, 5, 10)\n",
    "range = 10 # zero based, 1 means 2 days\n",
    "filing = filings[0] # select one filing\n",
    "\n",
    "\n",
    "# run the scraper\n",
    "df_list = []\n",
    "df_error = []\n",
    "start = start_date\n",
    "end = end_date\n",
    "while start_date <= end_date:\n",
    "    driver = setup_driver()\n",
    "    mid_date = start_date + timedelta(days=range) if start_date + timedelta(days=range) < end_date else end_date\n",
    "    df = perform_search(start_date, mid_date, filing)\n",
    "    if df is not None:\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        df_error.append(f\"{filing}_{start_date.strftime('%Y-%m-%d')}_{mid_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {mid_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    start_date = mid_date + timedelta(days=1)\n",
    "    driver.quit()\n",
    "if df_list and all(df is not None for df in df_list):\n",
    "    df = pd.concat(df_list)\n",
    "    # df.to_csv(f\"results/{filing}_combined.csv\", index=False)\n",
    "    print('='*50)\n",
    "    print('Scraping Completed')\n",
    "    print('Total length: ', len(df))\n",
    "if df_error:\n",
    "    print('Error dates found: ', len(df_error))\n",
    "    df_error = pd.DataFrame(df_error)\n",
    "    df_error.to_csv(f\"results/{filing}_error_dates.csv\", index=False)\n",
    "    print('Saved to: ', f\"results/{filing}_error_dates_{start.strftime('%Y-%m-%d')}_{end.strftime('%Y-%m-%d')}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "scraping Annual Financial Information and Operating Data from 2021-02-01 to 2021-02-10\n",
      "clicked on disclosures tab\n",
      "*** page loaded\n",
      "Large search result detected\n",
      "Applying filter: GO\n",
      "Window loaded\n",
      "Results for filter GO loaded\n",
      "Number of disclosures: 625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:   0%|          | 0/625 [00:00<?, ?rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  16%|█▌        | 100/625 [00:02<00:11, 46.45rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  32%|███▏      | 200/625 [00:06<00:14, 29.89rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  48%|████▊     | 300/625 [00:10<00:11, 27.53rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  64%|██████▍   | 400/625 [00:14<00:08, 27.30rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  80%|████████  | 500/625 [00:17<00:04, 26.87rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  96%|█████████▌| 600/625 [00:21<00:00, 27.19rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results: 100%|██████████| 625/625 [00:23<00:00, 26.22rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "number of rows: 625\n",
      "Applying filter: REV\n",
      "Window loaded\n",
      "Results for filter REV loaded\n",
      "Number of disclosures: 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:   0%|          | 0/732 [00:00<?, ?rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  14%|█▎        | 100/732 [00:02<00:13, 48.39rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  27%|██▋       | 200/732 [00:05<00:16, 31.62rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  41%|████      | 300/732 [00:09<00:14, 30.44rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  55%|█████▍    | 400/732 [00:13<00:11, 29.02rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  68%|██████▊   | 500/732 [00:16<00:08, 27.76rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  82%|████████▏ | 600/732 [00:20<00:04, 27.86rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:  96%|█████████▌| 700/732 [00:24<00:01, 27.84rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results: 100%|██████████| 732/732 [00:26<00:00, 27.62rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "number of rows: 732\n",
      "Applying filter: DB\n",
      "Window loaded\n",
      "Results for filter DB loaded\n",
      "Number of disclosures: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results:   0%|          | 0/39 [00:00<?, ?rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping results: 100%|██████████| 39/39 [00:00<00:00, 40.70rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "number of rows: 39\n",
      "********** results/Annual Financial Information and Operating Data/results_2021-02-01_2021-02-10.csv Saved\n",
      "Cleared the search\n",
      "==================================================\n",
      "Scraping Completed\n",
      "Total length:  1396\n"
     ]
    }
   ],
   "source": [
    "# New Version\n",
    "filings = [\n",
    "    \"Annual Financial Information and Operating Data\",\n",
    "    \"Audited Financial Statements or ACFR\",\n",
    "    \"Budget\",\n",
    "    \"Change in Accounting Standard\",\n",
    "    \"Change in Fiscal Year / Timing of Annual Disclosure\",\n",
    "    \"Consultant Reports\",\n",
    "    \"Failure to Provide Annual Financial Information as Required\",\n",
    "    \"Information Provided to Rating Agency, Credit / Liquidity Provider or Other Third Party\",\n",
    "    \"Interim / Additional Financial Information / Operating Data\",\n",
    "    \"Investment / Debt / Financial Policy\",\n",
    "    \"Other Financial / Operating Data\",\n",
    "    \"Quarterly / Monthly Financial Information\"\n",
    "]\n",
    "\n",
    "# Function to clear the search\n",
    "def clear_search():\n",
    "    try:\n",
    "        disclosures_tab = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.ID, 'disclosuresFilterLi'))\n",
    "            )\n",
    "        disclosures_tab.click()\n",
    "        disclosures_tab.click()\n",
    "\n",
    "        clear_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'clearSearchButton'))\n",
    "        )\n",
    "        clear_button.click()\n",
    "        random_sleep(1, 2)  # Wait a bit to ensure the page loads completely\n",
    "        print(\"Cleared the search\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to clear the search: {e}\")\n",
    "        debug_page(\"clear_search_error\")\n",
    "\n",
    "# Setup driver once outside the loop\n",
    "driver = setup_driver()\n",
    "\n",
    "# input date range and filings\n",
    "start_date = datetime(2021, 2, 1)\n",
    "end_date = datetime(2021, 2, 10)\n",
    "range = 10 # zero based, 1 means 2 days\n",
    "filing = filings[0] # select one filing\n",
    "\n",
    "# run the scraper\n",
    "start = start_date\n",
    "end = end_date\n",
    "df_list = []\n",
    "df_error = []\n",
    "start = start_date\n",
    "while start_date <= end_date:\n",
    "    mid_date = start_date + timedelta(days=range) if start_date + timedelta(days=range) < end_date else end_date\n",
    "    df = perform_search(start_date, mid_date, filing)\n",
    "    if df is not None:\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        df_error.append(f\"{filing}_{start_date.strftime('%Y-%m-%d')}_{mid_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"No results found from {start_date.strftime('%Y-%m-%d')} to {mid_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    start_date = mid_date + timedelta(days=1)\n",
    "    clear_search()  # Clear the search for the next iteration\n",
    "\n",
    "# Quit the driver after the loop\n",
    "driver.quit()\n",
    "\n",
    "if df_list and all(df is not None for df in df_list):\n",
    "    df = pd.concat(df_list)\n",
    "    # df.to_csv(f\"results/{filing}_combined.csv\", index=False)\n",
    "    print('='*50)\n",
    "    print('Scraping Completed')\n",
    "    print('Total length: ', len(df))\n",
    "if df_error:\n",
    "    print('Error dates found: ', len(df_error))\n",
    "    df_error = pd.DataFrame(df_error)\n",
    "    df_error.to_csv(f\"results/{filing}_error_dates_{start.strftime('%Y-%m-%d')}_{end.strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "    print('Saved to: ', f\"results/{filing}_error_dates_{start.strftime('%Y-%m-%d')}_{end.strftime('%Y-%m-%d')}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8AKqqnF_RWG"
   },
   "source": [
    "## Scraping detailed page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n3QHtKXB_Sq8"
   },
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def download_pdf(url, file_name):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/pdf',\n",
    "        'Referer': 'https://emma.msrb.org/' \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        response.raise_for_status() \n",
    "        if not os.path.exists('downloaded_pdfs'):\n",
    "            os.makedirs('downloaded_pdfs')\n",
    "        download_dir = os.path.join('downloaded_pdfs', file_name)\n",
    "        with open(download_dir, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded {file_name} successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {file_name}. Error: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to save {file_name}. Error: {e}\")\n",
    "        \n",
    "# get cusip text\n",
    "def get_cusip_text(img_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/pdf',\n",
    "        'Referer': 'https://emma.msrb.org/' \n",
    "    }\n",
    "    img_response = requests.get(img_url, headers=headers, stream=True)\n",
    "    img = Image.open(BytesIO(img_response.content))\n",
    "    cusip_text = pytesseract.image_to_string(img, config='--psm 7').strip()\n",
    "    return cusip_text\n",
    "\n",
    "# Scrape detail page\n",
    "def scrape_detail_page(link):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        # Handle the terms of use if it appears\n",
    "        if len(driver.find_elements(By.CLASS_NAME, 'contentAreaDisclaimer')) > 0:\n",
    "            # print('terms_of_use found')\n",
    "            handle_terms_of_use()\n",
    "        # print('term_of_use handled')\n",
    "        \n",
    "        # Wait for the detail page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'card-body'))\n",
    "        )\n",
    "        # print('find body')\n",
    "        # click on the page to clear instructions\n",
    "        body = driver.find_element(By.TAG_NAME, 'body')\n",
    "        ActionChains(driver).move_to_element(body).click().perform()\n",
    "        # print('clicked')\n",
    "        random_sleep(2, 4)\n",
    "        # Scrape the required information\n",
    "        \n",
    "        '''\n",
    "        Part one\n",
    "        '''\n",
    "        # Extract Filing Type\n",
    "    \n",
    "        filing_type = WebDriverWait(driver, 5).until(\n",
    "            EC.visibility_of_element_located((By.ID, 'discType'))\n",
    "        ).text.strip()\n",
    "        \n",
    "        # Extract Disclosure Details\n",
    "        disclosure_details_div = WebDriverWait(driver, 5).until(\n",
    "            EC.visibility_of_element_located((By.ID, 'leftBlueBoxDiv'))\n",
    "        )   \n",
    "        disclosure_details = disclosure_details_div.text\n",
    "        \n",
    "        # Extract Contact Information\n",
    "        contact_info_div = WebDriverWait(driver, 5).until(\n",
    "            EC.visibility_of_element_located((By.ID, 'rightBlueBoxDiv'))\n",
    "        )   \n",
    "        contact_info = contact_info_div.text\n",
    "        # print('part one done')\n",
    "        '''\n",
    "        Part two\n",
    "        '''\n",
    "        # Extract Document Link\n",
    "        document_links = []\n",
    "        document_names = []\n",
    "        \n",
    "        document_button = driver.find_element(By.XPATH, \"//a[@id='viewDoc']\")\n",
    "\n",
    "        # Check if multiple documents are available\n",
    "        if document_button.get_attribute('href'):\n",
    "            document_link = document_button.get_attribute('href')\n",
    "            document_name = document_link.split('/')[-1]\n",
    "            document_links.append(document_link)\n",
    "            document_names.append(document_name)\n",
    "            # download pdf\n",
    "            download_pdf(document_link, document_name)\n",
    "        else:\n",
    "            # Get the help attribute which contains the links\n",
    "            help_attribute = document_button.get_attribute('help')\n",
    "            # Parse the help attribute as HTML\n",
    "            soup = BeautifulSoup(help_attribute, 'html.parser')\n",
    "            # Find all the links in the help attribute\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "            # Download each link\n",
    "            for link in links:\n",
    "                document_name = link.split('/')[-1]\n",
    "                document_link = f\"https://emma.msrb.org{link}\"\n",
    "                document_links.append(document_link)\n",
    "                document_names.append(document_name)\n",
    "                # download pdf\n",
    "                download_pdf(document_link, document_name)\n",
    "        # print('download_pdf done')\n",
    "        \n",
    "        # Click the plus button to reveal the CUSIP image\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\"))\n",
    "        )\n",
    "        plus_button = driver.find_element(By.XPATH, \"//td[@class=' w30']/img[@class='detailsImg']\")\n",
    "        plus_button.click()\n",
    "        \n",
    "        # Wait for the CUSIP image to be visible\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//td/a/img[@data-cusip]\"))\n",
    "        )\n",
    "        \n",
    "        # print('cusip image found')\n",
    "        # Extract and recognize text from CUSIP image\n",
    "        cusip_img = driver.find_element(By.XPATH, \"//td/a/img[@data-cusip]\")\n",
    "        img_url = cusip_img.get_attribute('src')\n",
    "        cusip_text = get_cusip_text(img_url)\n",
    "\n",
    "        # print(cusip_text)\n",
    "        return {\n",
    "            'filing_type': filing_type,\n",
    "            'disclosure_details': disclosure_details,\n",
    "            'contact_info': contact_info,\n",
    "            'document_links': document_links,\n",
    "            'document_names': document_names,\n",
    "            'cusip_text': cusip_text\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping detail page {link}: {e}\")\n",
    "        debug_page(f\"{link.split('/')[-1]}_error\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_detail_pages_from_csv(file_path):\n",
    "    df = read_csv(file_path)\n",
    "    detail_data = []\n",
    "    failed_links = []\n",
    "    # set up tqdm progress bar\n",
    "    pbar = tqdm(total=len(df), desc='Scraping detail pages', unit='rows', dynamic_ncols=True)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        link = row['link']\n",
    "        print('='*50)\n",
    "        print('Sraping: ',link)\n",
    "        detail_info = scrape_detail_page(link)\n",
    "        if detail_info:\n",
    "            detail_data.append(detail_info)\n",
    "        else:\n",
    "            failed_links.append(link)\n",
    "        # save temp data\n",
    "        detail_df = pd.DataFrame(detail_data)\n",
    "        detail_df.to_csv(f'temp_detail_data_{file_path}', index=False)\n",
    "        failed_links_df = pd.DataFrame(failed_links)\n",
    "        failed_links_df.to_csv(f'temp_failed_links_{file_path}', index=False)\n",
    "        # Optional: Add a delay to mimic human behavior and avoid detection\n",
    "        random_sleep(2, 4)\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    driver.quit()\n",
    "    detail_df = pd.DataFrame(detail_data)\n",
    "    print(f\"Failed to scrape {len(failed_links)} links: {failed_links}\")\n",
    "\n",
    "    return detail_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "LAB6o2MVA53N",
    "outputId": "c6805f09-9f98-4354-d854-aad97d2ce75e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Sraping:  https://emma.msrb.org/MarketActivity/ContinuingDisclosureDetails/P21089859\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use the function to scrape details from the CSV\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m detail_df \u001b[38;5;241m=\u001b[39m scrape_detail_pages_from_csv(file_path)\n\u001b[1;32m     10\u001b[0m detail_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetail_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(detail_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[8], line 160\u001b[0m, in \u001b[0;36mscrape_detail_pages_from_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSraping: \u001b[39m\u001b[38;5;124m'\u001b[39m,link)\n\u001b[0;32m--> 160\u001b[0m detail_info \u001b[38;5;241m=\u001b[39m scrape_detail_page(link)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detail_info:\n\u001b[1;32m    162\u001b[0m     detail_data\u001b[38;5;241m.\u001b[39mappend(detail_info)\n",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m, in \u001b[0;36mscrape_detail_page\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_detail_page\u001b[39m(link):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         driver\u001b[38;5;241m.\u001b[39mget(link)\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# Handle the terms of use if it appears\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentAreaDisclaimer\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;66;03m# print('terms_of_use found')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# csv file\n",
    "# file_path = 'results_2024-04-24_2024-04-24.csv'\n",
    "\n",
    "file_path = 'results/Annual Financial Information and Operating Data/results_2021-01-01_2021-01-02.csv'\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = setup_driver()\n",
    "\n",
    "# Use the function to scrape details from the CSV\n",
    "detail_df = scrape_detail_pages_from_csv(file_path)\n",
    "detail_df.to_csv(f'detail_data_{file_path}', index=False)\n",
    "print(detail_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRIM4gK4_W6d"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
